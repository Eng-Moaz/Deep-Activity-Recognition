model:
  num_classes: 8

training:
  batch_size: 256
  learning_rate: 0.001
  dropout: 0.6
  optimizer: "AdamW"
  weight_decay: 0.0001
